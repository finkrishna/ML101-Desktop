{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (scaled): \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [1.  ]\n",
      " [0.89]]\n",
      "Predicted Output for Range: \n",
      "0[[0.88700292]\n",
      " [0.88321157]\n",
      " [0.8800145 ]]\n",
      "Loss for Range: \n",
      "00.004942685056204401\n",
      "Predicted Output for Range: \n",
      "1[[0.8894612 ]\n",
      " [0.88538228]\n",
      " [0.88250424]]\n",
      "Loss for Range: \n",
      "10.0047086752817406\n",
      "Predicted Output for Range: \n",
      "2[[0.8917256 ]\n",
      " [0.88738887]\n",
      " [0.8847988 ]]\n",
      "Loss for Range: \n",
      "20.00450258707102186\n",
      "Predicted Output for Range: \n",
      "3[[0.89381861]\n",
      " [0.88924993]\n",
      " [0.88692061]]\n",
      "Loss for Range: \n",
      "30.004320175249083613\n",
      "Predicted Output for Range: \n",
      "4[[0.89575928]\n",
      " [0.8909813 ]\n",
      " [0.88888871]]\n",
      "Loss for Range: \n",
      "40.004157974539274294\n",
      "Predicted Output for Range: \n",
      "5[[0.8975639 ]\n",
      " [0.89259654]\n",
      " [0.89071938]]\n",
      "Loss for Range: \n",
      "50.004013132890299227\n",
      "Predicted Output for Range: \n",
      "6[[0.89924647]\n",
      " [0.89410733]\n",
      " [0.89242666]]\n",
      "Loss for Range: \n",
      "60.003883285092419676\n",
      "Predicted Output for Range: \n",
      "7[[0.90081909]\n",
      " [0.89552379]\n",
      " [0.89402268]]\n",
      "Loss for Range: \n",
      "70.0037664558931230527\n",
      "Predicted Output for Range: \n",
      "8[[0.90229229]\n",
      " [0.89685475]\n",
      " [0.895518  ]]\n",
      "Loss for Range: \n",
      "80.0036609849741650164\n",
      "Predicted Output for Range: \n",
      "9[[0.90367527]\n",
      " [0.89810792]\n",
      " [0.89692187]]\n",
      "Loss for Range: \n",
      "90.0035654683115372635\n",
      "Predicted Output for Range: \n",
      "10[[0.9049761 ]\n",
      " [0.89929011]\n",
      " [0.89824239]]\n",
      "Loss for Range: \n",
      "100.003478711940827189\n",
      "Predicted Output for Range: \n",
      "11[[0.9062019 ]\n",
      " [0.90040733]\n",
      " [0.89948673]]\n",
      "Loss for Range: \n",
      "110.003399695207276997\n",
      "Predicted Output for Range: \n",
      "12[[0.90735898]\n",
      " [0.9014649 ]\n",
      " [0.90066124]]\n",
      "Loss for Range: \n",
      "120.0033275413331345072\n",
      "Predicted Output for Range: \n",
      "13[[0.90845293]\n",
      " [0.90246756]\n",
      " [0.90177156]]\n",
      "Loss for Range: \n",
      "130.0032614936779528313\n",
      "Predicted Output for Range: \n",
      "14[[0.90948876]\n",
      " [0.90341957]\n",
      " [0.90282275]]\n",
      "Loss for Range: \n",
      "140.0032008964631986708\n",
      "Predicted Output for Range: \n",
      "15[[0.91047093]\n",
      " [0.90432471]\n",
      " [0.9038193 ]]\n",
      "Loss for Range: \n",
      "150.003145179023749212\n",
      "Predicted Output for Range: \n",
      "16[[0.91140346]\n",
      " [0.90518642]\n",
      " [0.90476528]]\n",
      "Loss for Range: \n",
      "160.003093842865203568\n",
      "Predicted Output for Range: \n",
      "17[[0.91228997]\n",
      " [0.90600778]\n",
      " [0.90566434]]\n",
      "Loss for Range: \n",
      "170.0030464509680800177\n",
      "Predicted Output for Range: \n",
      "18[[0.91313372]\n",
      " [0.90679159]\n",
      " [0.90651978]]\n",
      "Loss for Range: \n",
      "180.0030026189025100605\n",
      "Predicted Output for Range: \n",
      "19[[0.91393768]\n",
      " [0.90754038]\n",
      " [0.9073346 ]]\n",
      "Loss for Range: \n",
      "190.002962007410377883\n",
      "Predicted Output for Range: \n",
      "20[[0.91470451]\n",
      " [0.90825645]\n",
      " [0.90811152]]\n",
      "Loss for Range: \n",
      "200.002924316183478264\n",
      "Predicted Output for Range: \n",
      "21[[0.91543668]\n",
      " [0.90894191]\n",
      " [0.90885301]]\n",
      "Loss for Range: \n",
      "210.002889278621616705\n",
      "Predicted Output for Range: \n",
      "22[[0.9161364 ]\n",
      " [0.90959868]\n",
      " [0.90956133]]\n",
      "Loss for Range: \n",
      "220.0028566573976357757\n",
      "Predicted Output for Range: \n",
      "23[[0.91680573]\n",
      " [0.91022851]\n",
      " [0.91023856]]\n",
      "Loss for Range: \n",
      "230.002826240690063531\n",
      "Predicted Output for Range: \n",
      "24[[0.91744651]\n",
      " [0.91083304]\n",
      " [0.91088658]]\n",
      "Loss for Range: \n",
      "240.0027978389706320925\n",
      "Predicted Output for Range: \n",
      "25[[0.91806048]\n",
      " [0.91141373]\n",
      " [0.91150715]]\n",
      "Loss for Range: \n",
      "250.0027712822549481072\n",
      "Predicted Output for Range: \n",
      "26[[0.91864921]\n",
      " [0.91197196]\n",
      " [0.91210185]]\n",
      "Loss for Range: \n",
      "260.002746417741349615\n",
      "Predicted Output for Range: \n",
      "27[[0.91921415]\n",
      " [0.91250898]\n",
      " [0.91267218]]\n",
      "Loss for Range: \n",
      "270.0027231077763970914\n",
      "Predicted Output for Range: \n",
      "28[[0.91975664]\n",
      " [0.91302598]\n",
      " [0.9132195 ]]\n",
      "Loss for Range: \n",
      "280.002701228096239455\n",
      "Predicted Output for Range: \n",
      "29[[0.92027793]\n",
      " [0.91352403]\n",
      " [0.91374506]]\n",
      "Loss for Range: \n",
      "290.002680666301821573\n",
      "Predicted Output for Range: \n",
      "30[[0.92077917]\n",
      " [0.91400413]\n",
      " [0.91425004]]\n",
      "Loss for Range: \n",
      "300.0026613205329865824\n",
      "Predicted Output for Range: \n",
      "31[[0.92126141]\n",
      " [0.91446721]\n",
      " [0.91473552]]\n",
      "Loss for Range: \n",
      "310.0026430983123076576\n",
      "Predicted Output for Range: \n",
      "32[[0.92172566]\n",
      " [0.91491415]\n",
      " [0.91520252]]\n",
      "Loss for Range: \n",
      "320.0026259155342185877\n",
      "Predicted Output for Range: \n",
      "33[[0.92217283]\n",
      " [0.91534575]\n",
      " [0.91565198]]\n",
      "Loss for Range: \n",
      "330.0026096955789075217\n",
      "Predicted Output for Range: \n",
      "34[[0.92260379]\n",
      " [0.91576277]\n",
      " [0.91608476]]\n",
      "Loss for Range: \n",
      "340.0025943685336532927\n",
      "Predicted Output for Range: \n",
      "35[[0.92301933]\n",
      " [0.91616591]\n",
      " [0.91650169]]\n",
      "Loss for Range: \n",
      "350.0025798705069482158\n",
      "Predicted Output for Range: \n",
      "36[[0.9234202 ]\n",
      " [0.91655582]\n",
      " [0.91690353]]\n",
      "Loss for Range: \n",
      "360.002566143022966831\n",
      "Predicted Output for Range: \n",
      "37[[0.92380711]\n",
      " [0.91693313]\n",
      " [0.917291  ]]\n",
      "Loss for Range: \n",
      "370.0025531324857893437\n",
      "Predicted Output for Range: \n",
      "38[[0.9241807 ]\n",
      " [0.91729842]\n",
      " [0.91766476]]\n",
      "Loss for Range: \n",
      "380.002540789704336279\n",
      "Predicted Output for Range: \n",
      "39[[0.9245416 ]\n",
      " [0.91765222]\n",
      " [0.91802545]]\n",
      "Loss for Range: \n",
      "390.0025290694702711294\n",
      "Predicted Output for Range: \n",
      "40[[0.92489038]\n",
      " [0.91799506]\n",
      " [0.91837365]]\n",
      "Loss for Range: \n",
      "400.002517930182223364\n",
      "Predicted Output for Range: \n",
      "41[[0.92522758]\n",
      " [0.9183274 ]\n",
      " [0.91870991]]\n",
      "Loss for Range: \n",
      "410.0025073335106092355\n",
      "Predicted Output for Range: \n",
      "42[[0.92555371]\n",
      " [0.9186497 ]\n",
      " [0.91903477]]\n",
      "Loss for Range: \n",
      "420.002497244098112824\n",
      "Predicted Output for Range: \n",
      "43[[0.92586926]\n",
      " [0.91896239]\n",
      " [0.9193487 ]]\n",
      "Loss for Range: \n",
      "430.002487629291555457\n",
      "Predicted Output for Range: \n",
      "44[[0.92617467]\n",
      " [0.91926588]\n",
      " [0.91965217]]\n",
      "Loss for Range: \n",
      "440.00247845890145031\n",
      "Predicted Output for Range: \n",
      "45[[0.92647037]\n",
      " [0.91956053]\n",
      " [0.91994562]]\n",
      "Loss for Range: \n",
      "450.0024697049860231126\n",
      "Predicted Output for Range: \n",
      "46[[0.92675676]\n",
      " [0.91984671]\n",
      " [0.92022947]]\n",
      "Loss for Range: \n",
      "460.0024613416568962554\n",
      "Predicted Output for Range: \n",
      "47[[0.92703423]\n",
      " [0.92012475]\n",
      " [0.92050409]]\n",
      "Loss for Range: \n",
      "470.002453344903989251\n",
      "Predicted Output for Range: \n",
      "48[[0.92730313]\n",
      " [0.92039499]\n",
      " [0.92076986]]\n",
      "Loss for Range: \n",
      "480.0024456924374959455\n",
      "Predicted Output for Range: \n",
      "49[[0.92756381]\n",
      " [0.92065773]\n",
      " [0.92102713]]\n",
      "Loss for Range: \n",
      "490.0024383635450631715\n",
      "Predicted Output for Range: \n",
      "50[[0.92781658]\n",
      " [0.92091324]\n",
      " [0.92127623]]\n",
      "Loss for Range: \n",
      "500.0024313389625241857\n",
      "Predicted Output for Range: \n",
      "51[[0.92806176]\n",
      " [0.92116182]\n",
      " [0.92151748]]\n",
      "Loss for Range: \n",
      "510.0024246007567386044\n",
      "Predicted Output for Range: \n",
      "52[[0.92829964]\n",
      " [0.92140371]\n",
      " [0.92175117]]\n",
      "Loss for Range: \n",
      "520.0024181322192620342\n",
      "Predicted Output for Range: \n",
      "53[[0.92853048]\n",
      " [0.92163918]\n",
      " [0.92197758]]\n",
      "Loss for Range: \n",
      "530.0024119177697184187\n",
      "Predicted Output for Range: \n",
      "54[[0.92875456]\n",
      " [0.92186844]\n",
      " [0.92219698]]\n",
      "Loss for Range: \n",
      "540.002405942867878467\n",
      "Predicted Output for Range: \n",
      "55[[0.92897211]\n",
      " [0.92209173]\n",
      " [0.92240963]]\n",
      "Loss for Range: \n",
      "550.002400193933560811\n",
      "Predicted Output for Range: \n",
      "56[[0.92918339]\n",
      " [0.92230926]\n",
      " [0.92261578]]\n",
      "Loss for Range: \n",
      "560.0023946582735728072\n",
      "Predicted Output for Range: \n",
      "57[[0.92938861]\n",
      " [0.92252123]\n",
      " [0.92281565]]\n",
      "Loss for Range: \n",
      "570.0023893240149941352\n",
      "Predicted Output for Range: \n",
      "58[[0.92958798]\n",
      " [0.92272784]\n",
      " [0.92300946]]\n",
      "Loss for Range: \n",
      "580.002384180044183867\n",
      "Predicted Output for Range: \n",
      "59[[0.92978173]\n",
      " [0.92292928]\n",
      " [0.92319743]]\n",
      "Loss for Range: \n",
      "590.0023792159509580827\n",
      "Predicted Output for Range: \n",
      "60[[0.92997003]\n",
      " [0.92312571]\n",
      " [0.92337976]]\n",
      "Loss for Range: \n",
      "600.0023744219774449586\n",
      "Predicted Output for Range: \n",
      "61[[0.93015308]\n",
      " [0.92331731]\n",
      " [0.92355663]]\n",
      "Loss for Range: \n",
      "610.0023697889711762894\n",
      "Predicted Output for Range: \n",
      "62[[0.93033105]\n",
      " [0.92350425]\n",
      " [0.92372824]]\n",
      "Loss for Range: \n",
      "620.0023653083420207208\n",
      "Predicted Output for Range: \n",
      "63[[0.93050412]\n",
      " [0.92368667]\n",
      " [0.92389476]]\n",
      "Loss for Range: \n",
      "630.0023609720226046315\n",
      "Predicted Output for Range: \n",
      "64[[0.93067244]\n",
      " [0.92386472]\n",
      " [0.92405635]]\n",
      "Loss for Range: \n",
      "640.0023567724319030862\n",
      "Predicted Output for Range: \n",
      "65[[0.93083618]\n",
      " [0.92403854]\n",
      " [0.92421317]]\n",
      "Loss for Range: \n",
      "650.0023527024417152958\n",
      "Predicted Output for Range: \n",
      "66[[0.93099548]\n",
      " [0.92420828]\n",
      " [0.92436539]]\n",
      "Loss for Range: \n",
      "660.002348755345767561\n",
      "Predicted Output for Range: \n",
      "67[[0.93115049]\n",
      " [0.92437405]\n",
      " [0.92451314]]\n",
      "Loss for Range: \n",
      "670.0023449248312122166\n",
      "Predicted Output for Range: \n",
      "68[[0.93130134]\n",
      " [0.92453599]\n",
      " [0.92465657]]\n",
      "Loss for Range: \n",
      "680.002341204952313786\n",
      "Predicted Output for Range: \n",
      "69[[0.93144816]\n",
      " [0.92469421]\n",
      " [0.92479581]]\n",
      "Loss for Range: \n",
      "690.0023375901061334775\n",
      "Predicted Output for Range: \n",
      "70[[0.93159109]\n",
      " [0.92484884]\n",
      " [0.92493099]]\n",
      "Loss for Range: \n",
      "700.002334075010041674\n",
      "Predicted Output for Range: \n",
      "71[[0.93173023]\n",
      " [0.92499997]\n",
      " [0.92506224]]\n",
      "Loss for Range: \n",
      "710.002330654680904001\n",
      "Predicted Output for Range: \n",
      "72[[0.93186572]\n",
      " [0.92514771]\n",
      " [0.92518968]]\n",
      "Loss for Range: \n",
      "720.002327324415800873\n",
      "Predicted Output for Range: \n",
      "73[[0.93199765]\n",
      " [0.92529218]\n",
      " [0.92531341]]\n",
      "Loss for Range: \n",
      "730.002324079774153874\n",
      "Predicted Output for Range: \n",
      "74[[0.93212613]\n",
      " [0.92543346]\n",
      " [0.92543356]]\n",
      "Loss for Range: \n",
      "740.0023209165611434673\n",
      "Predicted Output for Range: \n",
      "75[[0.93225127]\n",
      " [0.92557164]\n",
      " [0.92555023]]\n",
      "Loss for Range: \n",
      "750.002317830812313544\n",
      "Predicted Output for Range: \n",
      "76[[0.93237317]\n",
      " [0.92570683]\n",
      " [0.92566351]]\n",
      "Loss for Range: \n",
      "760.002314818779267131\n",
      "Predicted Output for Range: \n",
      "77[[0.93249192]\n",
      " [0.9258391 ]\n",
      " [0.92577351]]\n",
      "Loss for Range: \n",
      "770.002311876916366855\n",
      "Predicted Output for Range: \n",
      "78[[0.93260762]\n",
      " [0.92596855]\n",
      " [0.92588033]]\n",
      "Loss for Range: \n",
      "780.0023090018683605933\n",
      "Predicted Output for Range: \n",
      "79[[0.93272035]\n",
      " [0.92609524]\n",
      " [0.92598404]]\n",
      "Loss for Range: \n",
      "790.002306190458860434\n",
      "Predicted Output for Range: \n",
      "80[[0.93283019]\n",
      " [0.92621927]\n",
      " [0.92608475]]\n",
      "Loss for Range: \n",
      "800.002303439679608796\n",
      "Predicted Output for Range: \n",
      "81[[0.93293723]\n",
      " [0.9263407 ]\n",
      " [0.92618253]]\n",
      "Loss for Range: \n",
      "810.002300746680471369\n",
      "Predicted Output for Range: \n",
      "82[[0.93304155]\n",
      " [0.92645961]\n",
      " [0.92627747]]\n",
      "Loss for Range: \n",
      "820.002298108760101832\n",
      "Predicted Output for Range: \n",
      "83[[0.93314322]\n",
      " [0.92657606]\n",
      " [0.92636965]]\n",
      "Loss for Range: \n",
      "830.0022955233572278123\n",
      "Predicted Output for Range: \n",
      "84[[0.93324232]\n",
      " [0.92669013]\n",
      " [0.92645913]]\n",
      "Loss for Range: \n",
      "840.0022929880425115613\n",
      "Predicted Output for Range: \n",
      "85[[0.93333891]\n",
      " [0.92680188]\n",
      " [0.92654601]]\n",
      "Loss for Range: \n",
      "850.002290500510943205\n",
      "Predicted Output for Range: \n",
      "86[[0.93343308]\n",
      " [0.92691138]\n",
      " [0.92663033]]\n",
      "Loss for Range: \n",
      "860.002288058574727273\n",
      "Predicted Output for Range: \n",
      "87[[0.93352487]\n",
      " [0.92701867]\n",
      " [0.92671218]]\n",
      "Loss for Range: \n",
      "870.002285660156626925\n",
      "Predicted Output for Range: \n",
      "88[[0.93361436]\n",
      " [0.92712383]\n",
      " [0.92679162]]\n",
      "Loss for Range: \n",
      "880.0022833032837326493\n",
      "Predicted Output for Range: \n",
      "89[[0.93370161]\n",
      " [0.92722691]\n",
      " [0.92686871]]\n",
      "Loss for Range: \n",
      "890.0022809860816255406\n",
      "Predicted Output for Range: \n",
      "90[[0.93378667]\n",
      " [0.92732796]\n",
      " [0.92694352]]\n",
      "Loss for Range: \n",
      "900.002278706768906781\n",
      "Predicted Output for Range: \n",
      "91[[0.93386961]\n",
      " [0.92742705]\n",
      " [0.9270161 ]]\n",
      "Loss for Range: \n",
      "910.0022764636520678943\n",
      "Predicted Output for Range: \n",
      "92[[0.93395047]\n",
      " [0.92752421]\n",
      " [0.92708651]]\n",
      "Loss for Range: \n",
      "920.002274255120678089\n",
      "Predicted Output for Range: \n",
      "93[[0.93402932]\n",
      " [0.9276195 ]\n",
      " [0.92715481]]\n",
      "Loss for Range: \n",
      "930.002272079642866543\n",
      "Predicted Output for Range: \n",
      "94[[0.93410621]\n",
      " [0.92771296]\n",
      " [0.92722105]]\n",
      "Loss for Range: \n",
      "940.0022699357610797073\n",
      "Predicted Output for Range: \n",
      "95[[0.93418118]\n",
      " [0.92780465]\n",
      " [0.92728528]]\n",
      "Loss for Range: \n",
      "950.0022678220880948093\n",
      "Predicted Output for Range: \n",
      "96[[0.93425429]\n",
      " [0.92789461]\n",
      " [0.92734756]]\n",
      "Loss for Range: \n",
      "960.0022657373032721677\n",
      "Predicted Output for Range: \n",
      "97[[0.93432558]\n",
      " [0.92798288]\n",
      " [0.92740793]]\n",
      "Loss for Range: \n",
      "970.0022636801490307523\n",
      "Predicted Output for Range: \n",
      "98[[0.9343951 ]\n",
      " [0.92806951]\n",
      " [0.92746644]]\n",
      "Loss for Range: \n",
      "980.0022616494275315546\n",
      "Predicted Output for Range: \n",
      "99[[0.93446289]\n",
      " [0.92815453]\n",
      " [0.92752314]]\n",
      "Loss for Range: \n",
      "990.0022596439975557923\n",
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "[0.5 1. ]\n",
      "Output: \n",
      "[0.9354329]\n",
      "W1[[ 0.44074784 -0.13278637 -0.36422544]\n",
      " [ 3.47467795  0.29614391 -2.08836275]]\n",
      "W2[[1.19187253]\n",
      " [2.62676005]\n",
      " [0.49929012]]\n",
      "[0.004942685056204401, 0.0047086752817406, 0.00450258707102186, 0.004320175249083613, 0.004157974539274294, 0.004013132890299227, 0.003883285092419676, 0.0037664558931230527, 0.0036609849741650164, 0.0035654683115372635, 0.003478711940827189, 0.003399695207276997, 0.0033275413331345072, 0.0032614936779528313, 0.0032008964631986708, 0.003145179023749212, 0.003093842865203568, 0.0030464509680800177, 0.0030026189025100605, 0.002962007410377883, 0.002924316183478264, 0.002889278621616705, 0.0028566573976357757, 0.002826240690063531, 0.0027978389706320925, 0.0027712822549481072, 0.002746417741349615, 0.0027231077763970914, 0.002701228096239455, 0.002680666301821573, 0.0026613205329865824, 0.0026430983123076576, 0.0026259155342185877, 0.0026096955789075217, 0.0025943685336532927, 0.0025798705069482158, 0.002566143022966831, 0.0025531324857893437, 0.002540789704336279, 0.0025290694702711294, 0.002517930182223364, 0.0025073335106092355, 0.002497244098112824, 0.002487629291555457, 0.00247845890145031, 0.0024697049860231126, 0.0024613416568962554, 0.002453344903989251, 0.0024456924374959455, 0.0024383635450631715, 0.0024313389625241857, 0.0024246007567386044, 0.0024181322192620342, 0.0024119177697184187, 0.002405942867878467, 0.002400193933560811, 0.0023946582735728072, 0.0023893240149941352, 0.002384180044183867, 0.0023792159509580827, 0.0023744219774449586, 0.0023697889711762894, 0.0023653083420207208, 0.0023609720226046315, 0.0023567724319030862, 0.0023527024417152958, 0.002348755345767561, 0.0023449248312122166, 0.002341204952313786, 0.0023375901061334775, 0.002334075010041674, 0.002330654680904001, 0.002327324415800873, 0.002324079774153874, 0.0023209165611434673, 0.002317830812313544, 0.002314818779267131, 0.002311876916366855, 0.0023090018683605933, 0.002306190458860434, 0.002303439679608796, 0.002300746680471369, 0.002298108760101832, 0.0022955233572278123, 0.0022929880425115613, 0.002290500510943205, 0.002288058574727273, 0.002285660156626925, 0.0022833032837326493, 0.0022809860816255406, 0.002278706768906781, 0.0022764636520678943, 0.002274255120678089, 0.002272079642866543, 0.0022699357610797073, 0.0022678220880948093, 0.0022657373032721677, 0.0022636801490307523, 0.0022616494275315546, 0.0022596439975557923]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x116dd4780>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xt4XXWd7/H3Nzv3+6VJmyZpk9LQUm4tDQUKznARKSNj8QxiUUdGmUFHUBzHUfA8c86oxzPDXKyeEXWQi6hIQRSNDAMiBYEB2qa00DsNvaZNm7RNc2tz/54/9moJIZfdNulO9v68nifP3uu3f+u3v79ntflkXfba5u6IiIgMJSHaBYiIyPimoBARkWEpKEREZFgKChERGZaCQkREhqWgEBGRYSkoRERkWAoKEREZloJCRESGlRjtAkbDpEmTvLy8PNpliIhMKKtXrz7g7oUj9YuJoCgvL6empibaZYiITChmtjOSfhEdejKzRWa2xcxqzezOQV5PMbNHg9dXmFl5v9fuCtq3mNk1/dp3mNk6M1trZjX92vPN7Fkz2xo85kVSo4iIjI0Rg8LMQsA9wLXAHOAmM5szoNstQJO7zwSWAncH684BlgBnA4uA7wfjHXOFu89196p+bXcCz7l7JfBcsCwiIlESyR7FAqDW3be5exewDFg8oM9i4KHg+ePAVWZmQfsyd+909+1AbTDecPqP9RBwfQQ1iojIGIkkKEqA3f2W64K2Qfu4ew/QDBSMsK4DvzOz1WZ2a78+k929PhirHiiKbCoiIjIWIjmZbYO0DfwSi6H6DLfupe6+18yKgGfNbLO7vxhBPeE3DIfLrQDTpk2LdDURETlBkexR1AFl/ZZLgb1D9TGzRCAHODTcuu5+7LEBeIJ3DkntN7PiYKxioGGwotz9XnevcveqwsIRr+4SEZGTFElQrAIqzazCzJIJn5yuHtCnGrg5eH4DsNzDX51XDSwJroqqACqBlWaWYWZZAGaWAXwAWD/IWDcDvzm5qYmIyGgY8dCTu/eY2e3AM0AIeMDdN5jZN4Aad68G7gd+ama1hPcklgTrbjCzx4CNQA9wm7v3mtlk4Inw+W4SgZ+7+9PBW/4T8JiZ3QLsAj4yivN9l+c3N7B5Xyt/ffkZY/UWIiITnsXCd2ZXVVX5yXzg7v88uZGfvraTjd9YRChhsNMpIiKxy8xWD/h4wqDi+l5Ps6Zk0dnTx46D7dEuRURk3IrroDirOBuALftao1yJiMj4FddBMbMokwSDzfUt0S5FRGTciuugSE0KUTEpg03aoxARGVJcBwXA7OJsNu/THoWIyFDiPijOmpLF7kNHaevsiXYpIiLjUtwHxawpOqEtIjKcuA+K2VOyAHT4SURkCHEfFKV5aWSmJGqPQkRkCHEfFGbGrClZbK5XUIiIDCbugwLCh5827WshFm5nIiIy2hQUhIOitaOH+uaOaJciIjLuKCgIf5YCdEJbRGQwCgrCNwcE2KTzFCIi76GgALJTkyjJTdOVTyIig1BQBGZPydKhJxGRQSgoArOLs3i7sZ2O7t5olyIiMq4oKALnluTQ2+ds0i3HRUTeRUEROLc0F4D1e5qjXImIyPiioAhMzUklPyOZN+sUFCIi/SkoAmbGuSU5rNMehYjIuygo+jmvNIe39rdytEsntEVEjlFQ9HNuSQ59DhvrtVchInKMgqKf84IT2jpPISLyDgVFP5OzUyjMStF5ChGRfhQU/ZgZ55XksE57FCIixykoBjinJIfaxjbaO3uiXYqIyLigoBjgvNIc3GHDXn1CW0QEIgwKM1tkZlvMrNbM7hzk9RQzezR4fYWZlfd77a6gfYuZXTNgvZCZrTGzJ/u1/djMtpvZ2uBn7slP78SdW5IDwJt1h0/n24qIjFuJI3UwsxBwD3A1UAesMrNqd9/Yr9stQJO7zzSzJcDdwEfNbA6wBDgbmAr83szOdPdjH1S4A9gEZA94279z98dPZWInqyg7lSnZqTqhLSISiGSPYgFQ6+7b3L0LWAYsHtBnMfBQ8Pxx4Cozs6B9mbt3uvt2oDYYDzMrBT4I3Hfq0xhd55bqE9oiIsdEEhQlwO5+y3VB26B93L0HaAYKRlj3O8BXgL5B3vNbZvammS01s5QIahxV55XksK2xnZaO7tP91iIi404kQWGDtHmEfQZtN7PrgAZ3Xz3I63cBs4ELgXzgq4MWZXarmdWYWU1jY+OQxZ+MedPyAFizS+cpREQiCYo6oKzfcimwd6g+ZpYI5ACHhln3UuBDZraD8KGsK83sZwDuXu9hncCDBIeqBnL3e929yt2rCgsLI5hG5OZOyyXBYPXOplEdV0RkIookKFYBlWZWYWbJhE9OVw/oUw3cHDy/AVju7h60LwmuiqoAKoGV7n6Xu5e6e3kw3nJ3/wSAmRUHjwZcD6w/pRmehMyURGZPyWb1zkOn+61FRMadEa96cvceM7sdeAYIAQ+4+wYz+wZQ4+7VwP3AT82slvCexJJg3Q1m9hiwEegBbut3xdNQHjazQsKHrdYCnz3JuZ2SqvI8Hl9dR09vH4khfdxEROKXhf/wn9iqqqq8pqZmVMf8zdo93LFsLU9+/jLOCT5bISISS8xstbtXjdRPfyoPYf708AltnacQkXinoBhCSW4ak7NTFBQiEvcUFEMwM6qm5ysoRCTuKSiGMX96HnsOH6W++Wi0SxERiRoFxTB0nkJEREExrDlTs0lLCikoRCSuKSiGkRRK4PyyHAWFiMQ1BcUI5k/PY8PeFo506RvvRCQ+KShGcGF5Pr19zus7dYNAEYlPCooRXFieT2KC8crbB6JdiohIVCgoRpCRksj5Zbm88vbBaJciIhIVCooILDyjgHV7mmnVFxmJSBxSUETgkjMK6O1zVu3QbcdFJP4oKCJwwbQ8khMTeKVWh59EJP4oKCKQmhSianqezlOISFxSUETokhkFbKxvoam9K9qliIicVgqKCC2cWQDAa9u0VyEi8UVBEaHzSnNJTw7xqoJCROKMgiJCSaEEFlTk6zyFiMQdBcUJWHhGAbUNbTS0dES7FBGR00ZBcQIunTkJgBe36nYeIhI/FBQnYE5xNkVZKTy/pSHapYiInDYKihNgZlw+q5CX3mqkp7cv2uWIiJwWCooTdMWsIlo6elizW7cdF5H4oKA4QZdWTiIxwXh+sw4/iUh8UFCcoOzUJKrK83h+S2O0SxEROS0UFCfh8llFbKpvYV+zLpMVkdinoDgJV8wqAuAFXf0kInEgoqAws0VmtsXMas3szkFeTzGzR4PXV5hZeb/X7grat5jZNQPWC5nZGjN7sl9bRTDG1mDM5JOf3tg4c3ImU3NSeUGHn0QkDowYFGYWAu4BrgXmADeZ2ZwB3W4Bmtx9JrAUuDtYdw6wBDgbWAR8PxjvmDuATQPGuhtY6u6VQFMw9rhiZlw+u4iXaw/Q1aPLZEUktkWyR7EAqHX3be7eBSwDFg/osxh4KHj+OHCVmVnQvszdO919O1AbjIeZlQIfBO47NkiwzpXBGARjXn8yExtrV8wqoq2zR996JyIxL5KgKAF291uuC9oG7ePuPUAzUDDCut8BvgL0/5O8ADgcjDHUe40Ll84sIDUpgd9t2BftUkRExlQkQWGDtHmEfQZtN7PrgAZ3X30S7xXuaHarmdWYWU1j4+k/V5CenMgfn1nIMxv209c3aIkiIjEhkqCoA8r6LZcCe4fqY2aJQA5waJh1LwU+ZGY7CB/KutLMfgYcAHKDMYZ6LwDc/V53r3L3qsLCwgimMfoWnTOFfS0drK3Tp7RFJHZFEhSrgMrgaqRkwienqwf0qQZuDp7fACx3dw/alwRXRVUAlcBKd7/L3UvdvTwYb7m7fyJY5/lgDIIxf3MK8xtTV86eTFLIeGa9Dj+JSOwaMSiC8wW3A88QvkLpMXffYGbfMLMPBd3uBwrMrBb4EnBnsO4G4DFgI/A0cJu7947wll8FvhSMVRCMPS7lpCWx8IxJPL1hH+GMExGJPRYLv+Cqqqq8pqYmKu/9yMpd3PWrdfzXHe/jrOLsqNQgInIyzGy1u1eN1E+fzD5FV8+ZjBn8lw4/iUiMUlCcokmZKVxYnq/zFCISsxQUo2DR2VPYsr+VbY1t0S5FRGTUKShGwaJzpgDwn2/WR7kSEZHRp6AYBVNz01hQkc+v1+7R1U8iEnMUFKPk+rklvN3Yzoa9LdEuRURkVCkoRskHzy0mOZTAE2v2RLsUEZFRpaAYJTnpSVw+q5DqN/bSq3s/iUgMUVCMog/PK6GxtZNX3j4Q7VJEREaNgmIUXTG7iKzURH69ZtD7GIqITEgKilGUmhTiT84p5un19RztGumWViIiE4OCYpQtnjeV9q5ent20P9qliIiMCgXFKLu4ooCS3DQeW7V75M4iIhOAgmKUJSQYH72wjJdrD7DzYHu0yxEROWUKijHwkapSEgwe1V6FiMQABcUYKM5J44pZRfxidR3dvX3RLkdE5JQoKMbITQum0djayXObGqJdiojIKVFQjJHLZxUyOTuFZat2RbsUEZFToqAYI4mhBG6sKuMPbzWy5/DRaJcjInLSFBRj6MaqMgCWrdRehYhMXAqKMVSWn85Vs4v4+YpddHTrk9oiMjEpKMbYpy+t4GB7F9Vv6P5PIjIxKSjG2CVnFDBrchYPvLxd334nIhOSgmKMmRmfvqyczftaeXXbwWiXIyJywhQUp8HiuSXkZyTz4H/viHYpIiInTEFxGqQmhfj4RdP4/ab9uv+TiEw4CorT5BMXTycxwXjg5e3RLkVE5IQoKE6TydmpXD+3hGWrdtPY2hntckREIhZRUJjZIjPbYma1ZnbnIK+nmNmjwesrzKy832t3Be1bzOyaoC3VzFaa2RtmtsHMvt6v/4/NbLuZrQ1+5p76NMeHv778DLp7+7hfexUiMoGMGBRmFgLuAa4F5gA3mdmcAd1uAZrcfSawFLg7WHcOsAQ4G1gEfD8YrxO40t3PB+YCi8zs4n7j/Z27zw1+1p7SDMeRGYWZ/Mm5xfzstZ00H+mOdjkiIhGJZI9iAVDr7tvcvQtYBiwe0Gcx8FDw/HHgKjOzoH2Zu3e6+3agFljgYW1B/6TgJy4+ZHDbFTNp6+zhx6/siHYpIiIRiSQoSoD+38BTF7QN2sfde4BmoGC4dc0sZGZrgQbgWXdf0a/ft8zsTTNbamYpJzCfce+s4mzef1YRD76ynfbOnmiXIyIyokiCwgZpG/jX/1B9hlzX3XvdfS5QCiwws3OC1+8CZgMXAvnAVwctyuxWM6sxs5rGxsaRZzGO3HbFTA4f6eZnr+2MdikiIiOKJCjqgLJ+y6XAwBsXHe9jZolADnAoknXd/TDwAuFzGLh7fXBoqhN4kPChr/dw93vdvcrdqwoLCyOYxvgxb1oe76ucxH+8uI3WDp2rEJHxLZKgWAVUmlmFmSUTPjldPaBPNXBz8PwGYLmHb2xUDSwJroqqACqBlWZWaGa5AGaWBrwf2BwsFwePBlwPrD+VCY5XX/7ALA61d3HfS7oCSkTGt8SROrh7j5ndDjwDhIAH3H2DmX0DqHH3auB+4KdmVkt4T2JJsO4GM3sM2Aj0ALe5e28QBg8FV0AlAI+5+5PBWz5sZoWED1utBT47mhMeL84vy2XR2VO476VtfPKS6RRkxtSpGBGJIRYLdzStqqrympqaaJdxwmobWvnA0hf51KUV/P11A684FhEZW2a22t2rRuqnT2ZH0cyiLP7sglJ++upOfV2qiIxbCooo++LVZwKw9Nm3olyJiMjgFBRRVpKbxicvmc4vX69jXV1ztMsREXkPBcU48PmrKslPT+brv92gb8ETkXFHQTEO5KQl8eVrZlGzs4nfvlkf7XJERN5FQTFO3FhVxpzibP7xqU0c7eqNdjkiIscpKMaJUILxv/90DvXNHfzgD29HuxwRkeMUFOPIRTMKuO68Yn74h7fZ1tg28goiIqeBgmKc+V/XzSElMYH/+cR6ndgWkXFBQTHOFGWncue1s3l120EeX10X7XJERBQU49FNF06janoe33pqEwfa9P3aIhJdCopxKCHB+Mf/cS7tnT1888mN0S5HROKcgmKcqpycxecun8lv1u7l6fX6bIWIRI+CYhy7/cqZnFOSzdeeWE9jqw5BiUh0KCjGsaRQAktvnEtbZw93/epNXQUlIlGhoBjnKidn8ZVrZvH7TQ38okZXQYnI6aegmAA+fWkFF8/I5+u/3aAP4onIaaegmAASEoylH51LcmICt/18DR3duheUiJw+CooJojgnjW/fOJdN9S26ZFZETisFxQRyxewiPvPHM3h4xS5++8beaJcjInFCQTHBfPkDs5g/PY87f/kmW/e3RrscEYkDCooJJimUwPc+No+05ET+6ic1NB/pjnZJIhLjFBQTUHFOGj/8xAXsOXyU2x95nZ7evmiXJCIxTEExQVWV5/PNxefw0tYD3P305miXIyIxLDHaBcjJW7JgGpvqW/jRS9upmJTJxy6aFu2SRCQGKSgmuL+/bg47Dx3h73+znik5KVw5e3K0SxKRGKNDTxNcYiiBez52AWcVZ3Hbw2t4s+5wtEsSkRijoIgBGSmJPPAXF1KQmcynf7yKHQfao12SiMSQiILCzBaZ2RYzqzWzOwd5PcXMHg1eX2Fm5f1euyto32Jm1wRtqWa20szeMLMNZvb1fv0rgjG2BmMmn/o0Y19RVio//tQC+hw+ft8K9hw+Gu2SRCRGjBgUZhYC7gGuBeYAN5nZnAHdbgGa3H0msBS4O1h3DrAEOBtYBHw/GK8TuNLdzwfmAovM7OJgrLuBpe5eCTQFY0sEZhZl8pNPL6Clo5uP/+g1Glo7ol2SiMSASPYoFgC17r7N3buAZcDiAX0WAw8Fzx8HrjIzC9qXuXunu28HaoEFHnbsNqhJwY8H61wZjEEw5vUnObe4dE5JDj/+1IU0tHby5/et5FB7V7RLEpEJLpKgKAF291uuC9oG7ePuPUAzUDDcumYWMrO1QAPwrLuvCNY5HIwx1HsRrH+rmdWYWU1jY2ME04gf86fn86NPVrHjYDtL7n1V344nIqckkqCwQdoGftXaUH2GXNfde919LlAKLDCzcyJ8L4L173X3KnevKiwsHLL4eHXpzEk8+BcXsvvQUZbc+yr7W3QYSkROTiRBUQeU9VsuBQbeuvR4HzNLBHKAQ5Gs6+6HgRcIn8M4AOQGYwz1XhKhhTMn8dCnF7CvuYMb/+NV6pqORLskEZmAIgmKVUBlcDVSMuGT09UD+lQDNwfPbwCWe/gLnquBJcFVURVAJbDSzArNLBfAzNKA9wObg3WeD8YgGPM3Jz89WVCRz09uuYhD7V382Q9eYfO+lmiXJCITzIhBEZwvuB14BtgEPObuG8zsG2b2oaDb/UCBmdUCXwLuDNbdADwGbASeBm5z916gGHjezN4kHETPuvuTwVhfBb4UjFUQjC2nYP70PH7x2UsA+MgPX2Xl9kNRrkhEJhIL/xE/sVVVVXlNTU20yxj36pqO8MkHVlLXdJSlN87lg+cVR7skEYkiM1vt7lUj9dMns+NIaV46v/zsQs4tyeG2n7/Ovz+3lVj4Q0FExpaCIs7kZSTz8F9exIfnlfBvz77FFx9dS0d3b7TLEpFxTHePjUOpSSG+feP5zCzK5F+e2cK2xnZ+8IkLKM1Lj3ZpIjIOaY8iTpkZt10xM/zBvAPtXPfvL/PiW/rgooi8l4Iizl09ZzLVn7+MyVmp3PzgSr7z+7fo7dN5CxF5h4JCqJiUwRO3LeT6uSV85/dbuelHr1HfrLvPikiYgkIASE9O5Ns3ns+/feR81u9p5trvvsTT6+ujXZaIjAMKCjnOzPiz+aU8+fnLKM1L47M/e52/eXQtzUe6o12aiESRgkLeY0ZhJr/660v5wlWVVL+xl2u+8yLPb2mIdlkiEiUKChlUcmICX7r6TJ743EIyUxP51IOruGPZGg606ZblIvFGQSHDOq80l//8wmV84apKnlpXz/u//Qceq9lNn66MEokbCgoZUUpiiC9dfSZPfeF9nFGYyVcef5MbfvgK6/c0R7s0ETkNFBQSscrJWfziM5fwzzecx86DR/jT773M155Yp8NRIjFOQSEnJCHBuLGqjOVfvpybLynn0VW7ufxfXuCe52t1zyiRGKWgkJOSk5bEP3zobJ754h9x8Yx8/uWZLVz5ry/w6Kpd9PT2Rbs8ERlFCgo5JTOLMrnv5gv5+V9dRGF2Kl/95To+sPRFfvvGXp3wFokRCgoZFQvPmMSvP7eQe/98Pokh4/OPrOGa77xI9Rt7de8okQlO33Ano663z3lqXT3/vnwrb+1vY0ZhBp/5oxlcP6+ElMRQtMsTkUCk33CnoJAx09fnPL1hH99bXsvG+hYmZ6fwqUsruGnBNHLSkqJdnkjcU1DIuOHuvFx7gB/+4W3+u/Yg6ckhbphfyl8sLGdGYWa0yxOJWwoKGZc27G3mwf/eQfXavXT19vG+ykl84uLpXDW7iMSQTpmJnE4KChnXGls7eWTlLh5ZuYv65g6Kc1L5yPxSPlJVRlm+vpJV5HRQUMiE0NPbx3ObG3h4xS5e2hr+KtbLZk7ihvmlfGDOFNKSdfJbZKwoKGTC2XP4KL+o2c0vaurYc/gomSmJXHvOFK6fV8LFMwoIJVi0SxSJKQoKmbD6+pyVOw7xq9freGrdPto6eyjMSuGD5xbzp+cXM68sjwSFhsgpU1BITOjo7mX55gaq1+5l+ZYGunr6mJydwrXnFHPN2VO4sDxPJ8FFTpKCQmJOa0c3yzc38J9v1vPCW4109fSRm57EVbMnc/WcIi6rLCQzJTHaZYpMGAoKiWntnT28+FYjv9u4n+c27aelo4ekkHFRRQFXzC7ij88s5IzCDMx0iEpkKKMaFGa2CPguEALuc/d/GvB6CvATYD5wEPiou+8IXrsLuAXoBb7g7s+YWVnQfwrQB9zr7t8N+v8D8FdAYzD819z9qeHqU1DEt+7ePlbvbGL55gae27SftxvbASjJTeOPzizkfZWTuGRGAXkZyVGuVGR8GbWgMLMQ8BZwNVAHrAJucveN/fp8DjjP3T9rZkuAD7v7R81sDvAIsACYCvweOBMoAord/XUzywJWA9e7+8YgKNrc/V8jnayCQvrbfegIL25t5IUtjbz29kFaO3swg7OnZnPJjAIWnjGJCyvydZhK4l6kQRHJ/5QFQK27bwsGXgYsBjb267MY+Ifg+ePA9yy8z78YWObuncB2M6sFFrj7q0A9gLu3mtkmoGTAmCInpSw/nY9fNJ2PXzSdnt4+3qhr5uWtB3jl7QM89MpOfvTSdhIMzp6aw4KKfC4sz6eqPI9JmSnRLl1kXIokKEqA3f2W64CLhurj7j1m1gwUBO2vDVi3pP+KZlYOzANW9Gu+3cw+CdQAf+vuTQOLMrNbgVsBpk2bFsE0JB4lhhKYPz2P+dPzuOP9lXR09/L6ziZe23aQlTsO8bPXdnL/y9sBKC9IZ/70fC6Ynsu8sjxmTcnSZzdEiCwoBvufMvB41VB9hl3XzDKBXwJfdPeWoPkHwDeDft8E/g349HsGcb8XuBfCh56Gn4JIWGpSiIUzJ7Fw5iQAOnt6Wb+nmZodTdTsbOL5LQ388vU6ANKTQ5xbksPcslzOK83lvNIcSvPSdIJc4k4kQVEHlPVbLgX2DtGnzswSgRzg0HDrmlkS4ZB42N1/dayDu+8/9tzMfgQ8GelkRE5USmKI+dPzmT89n88QvtPt7kNHeX1XE2t2NfFGXfgmhl3B17vmpidxztQczi7J5uypOZw9NZvyggzteUhMiyQoVgGVZlYB7AGWAB8b0KcauBl4FbgBWO7ubmbVwM/N7NuET2ZXAiuD8xf3A5vc/dv9BzKzYnevDxY/DKw/uamJnDgzY1pBOtMK0rl+XvgoaVdPH5v3tbBuTzPr9zTzZl0zD7y8ne7e8I5sWlKIWVOyOKs4m9lTspg1JYvZU7LITddVVhIbRgyK4JzD7cAzhC+PfcDdN5jZN4Aad68m/Ev/p8HJ6kOEw4Sg32OET1L3ALe5e6+ZXQb8ObDOzNYGb3XsMth/NrO5hA897QA+M4rzFTlhyYkJwaGn3ONtXT19bG1oZcPeFjbVh3+eWlfPIyt3He9TlJXCmZOzqJycyZmTs5hZlMnMwkxdpisTjj5wJzJK3J39LZ1s3tfCln2tvLW/ja0NrWzd38bR7t7j/QoykjmjMJMzijKYMSmTikkZVBRmUJaXTnKibkcip89oXh4rIhEwM6bkpDIlJ5XLZxUdb+/rc/YcPkptYxu1+9t4u7GNbY3tPLNhP4fa37mgMJRglOSmMb0gnYpJGUzLT2d6QQbTC9Ipy0vXLdclahQUImMsIcEoy0+nLD+dK/oFCMDhI11sP9DOtsZ2dhxsZ8fBI+w40M4Ta/bQ2tHzrr5FWSnhcfLSKMtPpzQvjdK8dEpy0yjOTSUlUUEiY0NBIRJFuenJzJuWzLxpee9qd3cOH+lm56Ej7DzYzu5DR9h16Ag7Dx5h1Y4mqt/YS1+/o8Zm4SCZmptGSW4aU3PTmJqTSnFuGlNz0piSk0pBRrJuzy4nRUEhMg6ZGXkZyeRlJDO3LPc9r3f39rGvuYO6pqPUNR1hz+Gj7Gk6yt7mo6zf08zvNu6nq6fvXeskhxIoyk6hOCeVydmpTMkOHyYryk5lclYKk7PD7TrEJQMpKEQmoKRQwvHDWeGbILybu3OwvYu9h49S39zBvuYO9jYfZX9zB/XNHazb08zvN+2no7vvPetmpSRSmJ1CUVYKRVmpFGalhH8yw4+TMlOYlJVMQUaKPj8SJxQUIjHIzMK/0DNTOK908D7uTktHD/tbOmho6WR/Swf7WjpobO2koTXc9kbdYRpaOt911dY77wH56clMykyhIDOZgswUCjKSmZSZTH5GCvnHn4dDJTstUZ9qn6AUFCJxyszISUsiJy2JMydnDdu3vbOHhtZODrR1cqC1k8bjj10caOvkUHsX6+oOc6Cti7bOnkHHSEwwctOTKchIJi8jibz08KG1vPTgeXq4PffY8/QkslKTtNcyDigoRGREGSmJVKQkUjEpY8S+Hd29NB3p4mBbFwfbuzjU3snBti6ajnSQgEs+AAAGNUlEQVRxqL2bQ+2dNB3pZmtDG03tXRw+2k1v3+Cf5zKDnLQkctOSyElPJjctidz0YDktiey0cLAcC7z+P6lJCdqDGSUKChEZValJIYpz0ijOSYuof1+f09rRQ9ORcJgcPtJ9/PHwkS6ajnTTfLSbw0fD7TsOtnP4SDctHd0M93nhpFB4jyk7NRwo2WlJZKUmBsuJ77QHbVmpiWQdf0wkIzlRV4kFFBQiElUJCUZOehI56UmUM/IeyzG9fU5bR08QIl20HA0/7//T0hE8Bsu7Dx2hNWg7dq+uoZhBZso7IZKZkng8TDKDMMlKCbdnpiYdfz28HH4tIyWR9OTQhN+zUVCIyIQU6hcw00g/oXXdnc6ePlo6umk52kNrRzetHT20dvTQ0tE9yHK4T2NbJ9sPtIeXO3vecwnyYBIMMpLDoZGREgpCJURmECTvekwOBf2OtYVIT36nT3pyiJTE039ITUEhInHHzEhNCpGaFKJo+PP4w+rs6aWto4f2zl5aO8OB0t7ZQ1tnz6DP27veeX6gNXziv70rvDzSHs4xiQlGehAo6ckh/u+Hz+WiGe+9RHo0KShERE5SSmKIlMwQBZmnPlZnTy/tnb3HA6W9s+f4cltnD0e6esPBEjw/9piVmnTqbz4CBYWIyDiQkhgiJTFE/ji8Db3uaSwiIsNSUIiIyLAUFCIiMiwFhYiIDEtBISIiw1JQiIjIsBQUIiIyLAWFiIgMy3y42y9OEGbWCOw8ydUnAQdGsZyJIh7nHY9zhvicdzzOGU583tPdvXCkTjERFKfCzGrcvSradZxu8TjveJwzxOe843HOMHbz1qEnEREZloJCRESGpaCAe6NdQJTE47zjcc4Qn/OOxznDGM077s9RiIjI8LRHISIiw4rroDCzRWa2xcxqzezOaNczFsyszMyeN7NNZrbBzO4I2vPN7Fkz2xo85kW71tFmZiEzW2NmTwbLFWa2Ipjzo2Y2/m78f4rMLNfMHjezzcE2vyTWt7WZ/U3wb3u9mT1iZqmxuK3N7AEzazCz9f3aBt22Fvb/gt9tb5rZBafy3nEbFGYWAu4BrgXmADeZ2ZzoVjUmeoC/dfezgIuB24J53gk85+6VwHPBcqy5A9jUb/luYGkw5ybglqhUNba+Czzt7rOB8wnPP2a3tZmVAF8Aqtz9HCAELCE2t/WPgUUD2obattcClcHPrcAPTuWN4zYogAVArbtvc/cuYBmwOMo1jTp3r3f314PnrYR/cZQQnutDQbeHgOujU+HYMLNS4IPAfcGyAVcCjwddYnHO2cAfAfcDuHuXux8mxrc14W/qTDOzRCAdqCcGt7W7vwgcGtA81LZdDPzEw14Dcs2s+GTfO56DogTY3W+5LmiLWWZWDswDVgCT3b0ewmECFEWvsjHxHeArQF+wXAAcdveeYDkWt/cMoBF4MDjkdp+ZZRDD29rd9wD/CuwiHBDNwGpif1sfM9S2HdXfb/EcFDZIW8xeAmZmmcAvgS+6e0u06xlLZnYd0ODuq/s3D9I11rZ3InAB8AN3nwe0E0OHmQYTHJNfDFQAU4EMwoddBoq1bT2SUf33Hs9BUQeU9VsuBfZGqZYxZWZJhEPiYXf/VdC8/9iuaPDYEK36xsClwIfMbAfhQ4pXEt7DyA0OT0Bsbu86oM7dVwTLjxMOjlje1u8Htrt7o7t3A78CFhL72/qYobbtqP5+i+egWAVUBldHJBM+AVYd5ZpGXXBs/n5gk7t/u99L1cDNwfObgd+c7trGirvf5e6l7l5OeLsud/ePA88DNwTdYmrOAO6+D9htZrOCpquAjcTwtiZ8yOliM0sP/q0fm3NMb+t+htq21cAng6ufLgaajx2iOhlx/YE7M/sTwn9phoAH3P1bUS5p1JnZZcBLwDreOV7/NcLnKR4DphH+z/YRdx94omzCM7PLgS+7+3VmNoPwHkY+sAb4hLt3RrO+0WZmcwmfwE8GtgGfIvwHYcxuazP7OvBRwlf4rQH+kvDx+Jja1mb2CHA54TvE7gf+N/BrBtm2QWh+j/BVUkeAT7l7zUm/dzwHhYiIjCyeDz2JiEgEFBQiIjIsBYWIiAxLQSEiIsNSUIiIyLAUFCIiMiwFhYiIDEtBISIiw/r/+jZ92T4s2J0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# X = (hours studying, hours sleeping), y = score on test, xPredicted = 4 hours studying & 8 hours sleeping (input data for prediction)\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [100], [89]), dtype=float)\n",
    "\n",
    "xPredicted = np.array(([4, 8]), dtype=float)\n",
    "\n",
    "# scale units\n",
    "X = X / np.amax(X, axis=0)  # maximum of X array\n",
    "xPredicted = xPredicted / np.amax(\n",
    "    xPredicted,\n",
    "    axis=0)  # maximum of xPredicted (our input data for the prediction)\n",
    "y = y / 100  # max test score is 100\n",
    "LossArray = []\n",
    "Range = []\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        #parameters\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "\n",
    "        #weights\n",
    "        self.W1 = np.random.randn(\n",
    "            self.inputSize,\n",
    "            self.hiddenSize)  # (3x2) weight matrix from input to hidden layer\n",
    "        self.W2 = np.random.randn(\n",
    "            self.hiddenSize,\n",
    "            self.outputSize)  # (3x1) weight matrix from hidden to output layer\n",
    "\n",
    "    def forward(self, X):\n",
    "        #forward propagation through our network\n",
    "        self.z = np.dot(\n",
    "            X,\n",
    "            self.W1)  # dot product of X (input) and first set of 3x2 weights\n",
    "        self.z2 = self.sigmoid(self.z)  # activation function\n",
    "        self.z3 = np.dot(\n",
    "            self.z2, self.W2\n",
    "        )  # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "        o = self.sigmoid(self.z3)  # final activation function\n",
    "        return o\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        # activation function\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        #derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        # backward propgate through the network\n",
    "        self.o_error = y - o  # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(\n",
    "            o)  # applying derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(\n",
    "            self.W2.T\n",
    "        )  # z2 error: how much our hidden layer weights contributed to output error\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(\n",
    "            self.z2)  # applying derivative of sigmoid to z2 error\n",
    "\n",
    "        self.W1 += X.T.dot(\n",
    "            self.z2_delta)  # adjusting first set (input --> hidden) weights\n",
    "        self.W2 += self.z2.T.dot(\n",
    "            self.o_delta)  # adjusting second set (hidden --> output) weights\n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "    def saveWeights(self):\n",
    "        np.savetxt(\"w1.txt\", self.W1, fmt=\"%s\")\n",
    "        np.savetxt(\"w2.txt\", self.W2, fmt=\"%s\")\n",
    " \n",
    "    def predict(self):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "        print (\"Output: \\n\" + str(self.forward(xPredicted)))\n",
    "        print (\"W1\" + str(self.W1))\n",
    "        print (\"W2\" + str(self.W2))\n",
    "\n",
    "\n",
    "NN = Neural_Network()\n",
    "print (\"Input (scaled): \\n\" + str(X))\n",
    "print (\"Actual Output: \\n\" + str(y))\n",
    "for i in range(100):  # trains the NN 1,000 times\n",
    "    #print \" #\" + str(i) + \"\\n\"\n",
    "    #print (\"Input (scaled): \\n\" + str(X))\n",
    "    #print (\"Actual Output: \\n\" + str(y))\n",
    "    print (\"Predicted Output for Range: \\n\" + str(i) + str(NN.forward(X)))\n",
    "    print (\"Loss for Range: \\n\" + str(i) + str(np.mean(\n",
    "        np.square(y - NN.forward(X)))))  # mean sum squared loss\n",
    "    Loss = (np.mean(\n",
    "        np.square(y - NN.forward(X))))\n",
    "    #print \"\\n\"\n",
    "    NN.train(X, y)\n",
    "    Range +=[i]\n",
    "    LossArray += [Loss]\n",
    "\n",
    "\n",
    "NN.saveWeights()\n",
    "NN.predict()\n",
    "\n",
    "print (LossArray)\n",
    "print (Range)\n",
    "pyplot.plot(Range,LossArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
